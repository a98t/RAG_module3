{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d8a1b3-c7e9-4a6b-8d1c-2e0f9a4b7c0d",
   "metadata": {},
   "source": [
    "# Simple RAG Example with Weaviate and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-0e1f2a3b4c5d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates a simple implementation of the Retrieval-Augmented Generation (RAG) pattern. The goal is to build a question-answering system that leverages a vector database to provide context-aware answers from a Large Language Model (LLM).\n",
    "\n",
    "The process involves:\n",
    "1.  **Data Preparation**: Creating a small, factual dataset of scientific notes.\n",
    "2.  **Environment Setup**: Preparing the Docker environment and installing dependencies.\n",
    "3.  **Database Deployment**: Launching a Weaviate vector database instance using Docker.\n",
    "4.  **Embeddings & Ingestion**: Generating vector embeddings for our data using Azure OpenAI and loading it into Weaviate.\n",
    "5.  **RAG Experiment**: Executing a RAG pipeline:\n",
    "    - Expanding a user's question.\n",
    "    - Searching for relevant documents in Weaviate.\n",
    "    - Generating a final answer using an LLM augmented with the retrieved documents.\n",
    "6.  **Cleanup**: Removing the Docker container to free up system resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-4b8c-9d0e-1f2a3b4c5d6e",
   "metadata": {},
   "source": [
    "## 2. System Environment Preparation\n",
    "\n",
    "This section contains helper functions to interact with the underlying operating system (Linux or Windows with WSL) to manage Docker containers and file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-a7b8-4c9d-0e1f-2a3b4c5d6e7f",
   "metadata": {},
   "source": [
    "### 2.1. WSL and Shell Command Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e5f6a7-b8c9-4d0e-1f2a-3b4c5d6e7f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System: Windows\n",
      "‚úÖ Shell command helpers are defined.\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# --- Platform Detection ---\n",
    "system = platform.system()\n",
    "print(f\"Operating System: {system}\")\n",
    "\n",
    "# --- Shell Command Helpers ---\n",
    "def run_windows_command(command):\n",
    "    \"\"\"Executes a command in PowerShell on Windows.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"powershell\", \"-Command\", command],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"replace\"\n",
    "    )\n",
    "    return {\n",
    "        \"returncode\": result.returncode,\n",
    "        \"stdout\": result.stdout.strip(),\n",
    "        \"stderr\": result.stderr.strip(),\n",
    "        \"success\": result.returncode == 0\n",
    "    }\n",
    "\n",
    "def run_linux_command(command):\n",
    "    \"\"\"Executes a command in a standard Linux/macOS shell.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        command,\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"replace\"\n",
    "    )\n",
    "    return {\n",
    "        \"returncode\": result.returncode,\n",
    "        \"stdout\": result.stdout.strip(),\n",
    "        \"stderr\": result.stderr.strip(),\n",
    "        \"success\": result.returncode == 0\n",
    "    }\n",
    "\n",
    "def run_shell_command(command):\n",
    "    \"\"\"Universal function to run a shell command, detecting the platform.\"\"\"\n",
    "    if system == \"Windows\":\n",
    "        return run_windows_command(command)\n",
    "    else:\n",
    "        return run_linux_command(command)\n",
    "\n",
    "print(\"‚úÖ Shell command helpers are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-c9d0-4e1f-2a3b-4c5d6e7f8a9b",
   "metadata": {},
   "source": [
    "### 2.2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a7b8c9-d0e1-4f2a-3b4c-5d6e7f8a9b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Required libraries have been installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!\"{sys.executable}\" -m pip install -q weaviate-client==4.18.1 langchain~=0.3.0 langchain-openai~=0.2.0 python-dotenv~=1.0.0 pandas~=2.2.0\n",
    "\n",
    "print(\"‚úÖ Required libraries have been installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571abdd1-7ca4-4f78-a133-9592e4b414d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "‚úÖ Extra libraries for local models run have been installed.\n",
      "‚úÖ Extra libraries for local models run have been installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!\"{sys.executable}\" -m pip install -U -q sentence-transformers accelerate\n",
    "\n",
    "print(\"‚úÖ Extra libraries for local models run have been installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-e1f2-4a3b-4c5d-6e7f8a9b0c1d",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set up the necessary configurations for Weaviate and Azure OpenAI. \n",
    "\n",
    "**Action Required**: You must create a `.env` file in the same directory as this notebook and add your Azure OpenAI credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1-f2a3-4b4c-5d6e-7f8a9b0c1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Created a template .env file. Please fill it with your Azure credentials.\n",
      "‚úÖ Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# --- DIAL ML MODELS CONFIGURATION ---\n",
    "# Create a dummy .env file if it doesn't exist for demonstration purposes\n",
    "if not os.path.exists('.env'):\n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(\"AZURE_OPENAI_API_KEY='YOUR_AZURE_OPENAI_KEY'\\n\")\n",
    "        f.write(\"AZURE_OPENAI_ENDPOINT='YOUR_AZURE_OPENAI_ENDPOINT'\\n\")\n",
    "        f.write(\"AZURE_OPENAI_API_VERSION='2024-02-01'\\n\")\n",
    "        f.write(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT='YOUR_EMBEDDING_DEPLOYMENT_NAME'\\n\")\n",
    "        f.write(\"AZURE_OPENAI_CHAT_DEPLOYMENT='YOUR_CHAT_DEPLOYMENT_NAME'\\n\")\n",
    "    print(\"‚ö†Ô∏è Created a template .env file. Please fill it with your Azure credentials.\")\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# --- ALTERNATIVE EMBEDDINGS CONFIGURATION ---\n",
    "# Set to True to use a local model (free, runs on CPU/GPU)\n",
    "# Set to False to use Azure OpenAI (requires an API key and funds in the account)\n",
    "USE_LOCAL_EMBEDDINGS = True\n",
    "\n",
    "# Embeddingd model for local run.\n",
    "# If you have access to Gemma (you logged in via huggingface-cli), use: \"google/embeddinggemma-300m\" (768 dimensions)\n",
    "# If you don't have access or encounter errors, use the standard one: \"all-MiniLM-L6-v2\" (384 dimensions)\n",
    "LOCAL_EMBEDDING_MODEL_NAME = \"google/embeddinggemma-300m\"\n",
    "# LOCAL_EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\" # Uncomment if Gemma doesn't work\n",
    "\n",
    "\n",
    "# --- ALTERNATIVE LLM (CHAT) CONFIGURATION ---\n",
    "# Set to True to use a local LLM via Hugging Face Pipeline (CPU optimized)\n",
    "USE_LOCAL_LLM = True\n",
    "\n",
    "# Text generation model for local run.\n",
    "LOCAL_LLM_MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "\n",
    "\n",
    "# --- VECTOR DATABASE CONFIGURATION ---\n",
    "WEAVIATE_CONTAINER_NAME = \"simple-rag-weaviate\"\n",
    "WEAVIATE_IMAGE = \"semitechnologies/weaviate:1.33.7\"\n",
    "WEAVIATE_HTTP_PORT = 8080\n",
    "WEAVIATE_GRPC_PORT = 50051\n",
    "\n",
    "print(\"‚úÖ Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-a3b4-4c5d-6e7f-8a9b0c1d2e3f",
   "metadata": {},
   "source": [
    "## 4. Data Generation\n",
    "\n",
    "Here we generate 25 simple, factual notes across 5 different topics. Each note contains a specific detail (like a number, a name, or a technical term) to make it uniquely identifiable during the retrieval phase of our RAG experiment. This ensures we are testing the retrieval mechanism, not just the general knowledge of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-b4c5-4d6e-7f8a-9b0c1d2e3f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 25 documents across 5 topics.\n"
     ]
    }
   ],
   "source": [
    "documents_data = [\n",
    "    # Topic 1: Origins & Structure of the Premier League\n",
    "    {\n",
    "        \"title\": \"Birth of the Premier League\",\n",
    "        \"content\": \"The English Premier League (EPL) was founded in 1992 when clubs from the old First Division broke away from the Football League to take advantage of more lucrative TV rights. The league started with 22 teams but later reduced to 20. It quickly became one of the most watched sports competitions in the world.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Promotion and Relegation\",\n",
    "        \"content\": \"The Premier League uses a promotion and relegation system with the English Football League Championship. Each season, the bottom three clubs in the Premier League are relegated to the Championship, while the top two Championship clubs and the playoff winner are promoted. This system keeps competition intense at both ends of the table.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Points and League Table\",\n",
    "        \"content\": \"Teams in the Premier League earn three points for a win, one point for a draw, and none for a loss. The league table is ranked by total points, then goal difference, and then goals scored. If clubs are still level, head-to-head records and, ultimately, a playoff can be used to break ties in extreme cases.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"European Qualification\",\n",
    "        \"content\": \"Top Premier League clubs qualify for European competitions like the UEFA Champions League and Europa League. Typically, the top four teams enter the Champions League, while the next positions and domestic cup winners can earn Europa League or Conference League spots. This adds extra stakes beyond the domestic title race.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Financial Powerhouse\",\n",
    "        \"content\": \"The Premier League generates billions in revenue through broadcasting, sponsorship, and matchday income. TV rights are sold worldwide, and income is shared among clubs using a formula that rewards league position and appearances. This financial strength attracts elite players and coaches from around the globe.\"\n",
    "    },\n",
    "\n",
    "    # Topic 2: Iconic Moments in Premier League History\n",
    "    {\n",
    "        \"title\": \"‚ÄúAguerooooo‚Äù Title Winner\",\n",
    "        \"content\": \"In the 2011‚Äì12 season, Manchester City won their first Premier League title with a dramatic last-minute goal by Sergio Ag√ºero against Queens Park Rangers. The goal, scored deep into stoppage time, snatched the title away from Manchester United on goal difference. It is widely regarded as one of the most dramatic moments in league history.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"The Invincibles\",\n",
    "        \"content\": \"Arsenal‚Äôs 2003‚Äì04 team, nicknamed ‚ÄúThe Invincibles,‚Äù completed the entire Premier League season unbeaten. They recorded 26 wins and 12 draws, finishing with 90 points and a +47 goal difference. This achievement is considered one of the greatest team performances in modern football.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Leicester‚Äôs Miracle Season\",\n",
    "        \"content\": \"In 2015‚Äì16, Leicester City shocked the world by winning the Premier League after starting the season as relegation candidates and 5000‚Äì1 outsiders with bookmakers. Led by manager Claudio Ranieri and stars like Jamie Vardy and Riyad Mahrez, Leicester‚Äôs disciplined defending and rapid counter-attacks stunned more established clubs.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"The Battle of Old Trafford\",\n",
    "        \"content\": \"In 2003, a tense match between Manchester United and Arsenal ended in a mass confrontation known as the ‚ÄúBattle of Old Trafford.‚Äù A missed penalty, late challenges, and heated arguments led to multiple bans and fines. The incident highlighted the fierce rivalry between the two clubs during the early 2000s.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Record-Breaking Centurions\",\n",
    "        \"content\": \"Manchester City‚Äôs 2017‚Äì18 side became the first Premier League team to reach 100 points in a season. They scored 106 goals, won 32 of 38 matches, and set records for away wins and goal difference. Their possession-based, attacking style under Pep Guardiola defined a new standard of dominance.\"\n",
    "    },\n",
    "\n",
    "    # Topic 3: Football Analytics in the Premier League\n",
    "    {\n",
    "        \"title\": \"Expected Goals (xG)\",\n",
    "        \"content\": \"Expected goals, or xG, is a statistical metric that estimates the probability of a shot becoming a goal based on factors like shot location, body part, and type of assist. Premier League clubs use xG to evaluate performance beyond raw scorelines. A team consistently outperforming its xG may be finishing exceptionally well‚Äîor riding its luck.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Pressing and PPDA\",\n",
    "        \"content\": \"Pressing intensity is often measured with metrics like PPDA (Passes Allowed Per Defensive Action). A low PPDA indicates that a team allows few passes before applying defensive pressure, reflecting an aggressive pressing style. High-press teams such as those managed by J√ºrgen Klopp have helped make pressing statistics mainstream in Premier League analysis.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Heatmaps and Player Positioning\",\n",
    "        \"content\": \"Data providers create heatmaps to visualize where players spend most of their time on the pitch. These graphics show zones of high activity and can reveal roles like inverted full-backs or roaming playmakers. Coaches use heatmaps to adjust tactics and identify positional weaknesses in opponents.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Set-Piece Analysis\",\n",
    "        \"content\": \"Premier League clubs devote significant analytics resources to set pieces, such as corners and free-kicks. Analysts study delivery patterns, blocking runs, and opponent marking schemes to design routines that create high-quality chances. Some teams hire specialist set-piece coaches to gain an edge in these high-leverage moments.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Wearables and Tracking Data\",\n",
    "        \"content\": \"Players often wear GPS vests and tracking devices during training and matches. These systems collect data on total distance covered, sprint counts, and high-intensity efforts. Sports scientists combine this information with match analytics to manage fatigue, reduce injury risk, and individualize training loads.\"\n",
    "    },\n",
    "\n",
    "    # Topic 4: Tactics and Playing Styles\n",
    "    {\n",
    "        \"title\": \"The 4-3-3 and Wide Wingers\",\n",
    "        \"content\": \"The 4-3-3 formation has become a staple in the Premier League, emphasizing width and fluid front lines. Wide forwards cut inside to shoot, while full-backs overlap to provide crossing options. This system allows teams to press high and quickly surround the ball when possession is lost.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Low Blocks and Counter-Attacks\",\n",
    "        \"content\": \"Many underdog Premier League teams use a low defensive block, sitting deep near their own penalty area to deny space. When they win the ball, they launch fast counter-attacks using quick forwards and long passes into space. This style can frustrate possession-heavy giants and lead to shock upsets.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"False Nines and Fluid Forwards\",\n",
    "        \"content\": \"A false nine is a forward who frequently drops into midfield instead of staying near the opposition center-backs. This movement pulls defenders out of position and opens space for wingers or midfielders to run into. Several Premier League managers have used false nines to create unpredictable attacking patterns.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Build-Up from the Back\",\n",
    "        \"content\": \"Modern Premier League teams often build attacks from the goalkeeper and center-backs instead of kicking long. Players form passing triangles, and defensive midfielders drop deep to receive the ball under pressure. This approach requires technically skilled defenders and a clear positional structure to avoid dangerous turnovers.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Press-Resistant Midfielders\",\n",
    "        \"content\": \"Press-resistant midfielders are players who can receive the ball under pressure, turn away from opponents, and keep possession. In the Premier League‚Äôs high-tempo environment, such players are vital for progressing the ball through the middle third. Their ability to evade presses can completely change how a team advances up the pitch.\"\n",
    "    },\n",
    "\n",
    "    # Topic 5: Fan Culture and Stadium Atmosphere\n",
    "    {\n",
    "        \"title\": \"Home Advantage and Atmosphere\",\n",
    "        \"content\": \"Premier League stadiums are known for their loud, intense atmospheres, which can boost the home team‚Äôs performance. Chants, flags, and coordinated displays create a sense of intimidation for visiting players. The psychological effect of tens of thousands of supporters is one reason home advantage remains significant in football.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Club Anthems and Chants\",\n",
    "        \"content\": \"Many Premier League clubs have iconic songs or anthems associated with them. Fans of Liverpool sing ‚ÄúYou‚Äôll Never Walk Alone‚Äù before kick-off, while other clubs have their own traditional chants and melodies. These songs help create a shared identity and emotional connection between supporters and the team.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Matchday Rituals\",\n",
    "        \"content\": \"Supporters often follow specific rituals on matchdays, such as visiting the same pub, walking a particular route to the stadium, or wearing lucky scarves. These habits become part of the club‚Äôs culture and are passed down between generations. For many fans, the entire day‚Äînot just the 90 minutes‚Äîis a meaningful experience.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Rivalries and Local Identity\",\n",
    "        \"content\": \"Derby matches, like the North London derby or the Manchester derby, reflect local pride and historical tension between clubs. Fans see these fixtures as more than just games; they are battles for bragging rights within cities and communities. The emotional stakes make these matches some of the most intense in the league.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Global Fanbase\",\n",
    "        \"content\": \"The Premier League has a massive international following, with supporters‚Äô clubs on every continent. Fans who may never visit the stadium still wake up early or stay up late to watch live broadcasts. Social media and streaming platforms help create online communities that share reactions, memes, and analysis in real time.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(documents_data)} documents across 5 topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-4e7f-8a9b-0c1d2e3f4a5b",
   "metadata": {},
   "source": [
    "## 5. Docker Environment Setup\n",
    "\n",
    "We will now start the Weaviate database using a Docker container. The following cells will check for Docker, pull the required image, and run the container with the correct port mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a3b4c5-d6e7-4f8a-9b0c-1d2e3f4a5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stopping and removing any existing container named 'simple-rag-weaviate' ---\n",
      "Cleanup complete.\n",
      "\n",
      "--- Starting Weaviate container 'simple-rag-weaviate' ---\n",
      "Cleanup complete.\n",
      "\n",
      "--- Starting Weaviate container 'simple-rag-weaviate' ---\n",
      "‚ùå Failed to start Weaviate container.\n",
      "Error: docker: Error response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint simple-rag-weaviate (49ee0c53358a79053bf9b7cd67664b8dea3cce0b5690942fc438c3fba4dba0c2): Bind for 0.0.0.0:8080 failed: port is already allocated\n",
      "\n",
      "Run 'docker run --help' for more information\n",
      "\n",
      "Troubleshooting:\n",
      "1. Make sure Docker Desktop is running\n",
      "2. Check if port 8080 is already in use\n",
      "3. Try running 'docker ps' to see active containers\n",
      "\n",
      "--- Weaviate Container Stats ---\n",
      "CONTAINER ID   NAME                  CPU %     MEM USAGE / LIMIT   MEM %     NET I/O   BLOCK I/O   PIDS\n",
      "71c432acd5ab   simple-rag-weaviate   0.00%     0B / 0B             0.00%     0B / 0B   0B / 0B     0\n",
      "‚ùå Failed to start Weaviate container.\n",
      "Error: docker: Error response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint simple-rag-weaviate (49ee0c53358a79053bf9b7cd67664b8dea3cce0b5690942fc438c3fba4dba0c2): Bind for 0.0.0.0:8080 failed: port is already allocated\n",
      "\n",
      "Run 'docker run --help' for more information\n",
      "\n",
      "Troubleshooting:\n",
      "1. Make sure Docker Desktop is running\n",
      "2. Check if port 8080 is already in use\n",
      "3. Try running 'docker ps' to see active containers\n",
      "\n",
      "--- Weaviate Container Stats ---\n",
      "CONTAINER ID   NAME                  CPU %     MEM USAGE / LIMIT   MEM %     NET I/O   BLOCK I/O   PIDS\n",
      "71c432acd5ab   simple-rag-weaviate   0.00%     0B / 0B             0.00%     0B / 0B   0B / 0B     0\n"
     ]
    }
   ],
   "source": [
    "# First, ensure no old container with the same name is running\n",
    "print(f\"--- Stopping and removing any existing container named '{WEAVIATE_CONTAINER_NAME}' ---\")\n",
    "\n",
    "# Windows PowerShell and Linux use different error redirection\n",
    "if platform.system() == \"Windows\":\n",
    "    # PowerShell: suppress errors for stop/rm commands (they fail if container doesn't exist)\n",
    "    stop_command = f\"docker stop {WEAVIATE_CONTAINER_NAME} 2>&1 | Out-Null; docker rm {WEAVIATE_CONTAINER_NAME} 2>&1 | Out-Null\"\n",
    "else:\n",
    "    stop_command = f\"docker stop {WEAVIATE_CONTAINER_NAME} 2>/dev/null; docker rm {WEAVIATE_CONTAINER_NAME} 2>/dev/null\"\n",
    "\n",
    "run_shell_command(stop_command)\n",
    "print(\"Cleanup complete.\")\n",
    "\n",
    "# Now, run the new Weaviate container\n",
    "print(f\"\\n--- Starting Weaviate container '{WEAVIATE_CONTAINER_NAME}' ---\")\n",
    "run_command = (\n",
    "    f\"docker run -d \"\n",
    "    f\"--name {WEAVIATE_CONTAINER_NAME} \"\n",
    "    f\"-p {WEAVIATE_HTTP_PORT}:{WEAVIATE_HTTP_PORT} \"\n",
    "    f\"-p {WEAVIATE_GRPC_PORT}:{WEAVIATE_GRPC_PORT} \"\n",
    "    f\"-e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \"\n",
    "    f\"-e PERSISTENCE_DATA_PATH=/var/lib/weaviate \"\n",
    "    f\"-e DEFAULT_VECTORIZER_MODULE=none \"\n",
    "    f\"-e ENABLE_MODULES='' \"\n",
    "    f\"-e CLUSTER_HOSTNAME=node1 \"\n",
    "    f\"{WEAVIATE_IMAGE}\"\n",
    ")\n",
    "\n",
    "result = run_shell_command(run_command)\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"‚úÖ Weaviate container started successfully.\")\n",
    "    container_id = result['stdout'].strip()\n",
    "    if container_id and len(container_id) >= 12:\n",
    "        print(f\"Container ID: {container_id[:12]}\")\n",
    "    print(\"Waiting a few seconds for the service to initialize...\")\n",
    "    import time\n",
    "    time.sleep(10) # Give Weaviate time to start up\n",
    "else:\n",
    "    print(\"‚ùå Failed to start Weaviate container.\")\n",
    "    print(f\"Error: {result['stderr']}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure Docker Desktop is running\")\n",
    "    print(\"2. Check if port 8080 is already in use\")\n",
    "    print(\"3. Try running 'docker ps' to see active containers\")\n",
    "\n",
    "# Display container statistics\n",
    "print(\"\\n--- Weaviate Container Stats ---\")\n",
    "stats_result = run_shell_command(f\"docker stats {WEAVIATE_CONTAINER_NAME} --no-stream\")\n",
    "if stats_result[\"success\"]:\n",
    "    print(stats_result[\"stdout\"])\n",
    "else:\n",
    "    # If stats fail, just check if container is running\n",
    "    ps_result = run_shell_command(f\"docker ps --filter name={WEAVIATE_CONTAINER_NAME}\")\n",
    "    if ps_result[\"success\"]:\n",
    "        print(ps_result[\"stdout\"])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not retrieve container status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-4a9b-0c1d-2e3f4a5b6c7d",
   "metadata": {},
   "source": [
    "## 6. Embeddings and Data Ingestion\n",
    "\n",
    "In this section, we will:\n",
    "1.  Set up the LangChain clients for Azure OpenAI (for both embeddings and chat).\n",
    "2.  Generate vector embeddings for each of our 25 documents.\n",
    "3.  Connect to our Weaviate instance.\n",
    "4.  Define a data schema (a \"collection\") in Weaviate.\n",
    "5.  Batch-insert all documents and their vectors into the collection.\n",
    "\n",
    "**NOTE**: The loacal models examples have beed added as alternative, \n",
    "make sense to split this cell on two: clietns and data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7-f8a9-4b0c-1d2e-3f4a5b6c7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AT_Files\\AI Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Setting up AI clients ---\n",
      "üì• Loading local embedding model: google/embeddinggemma-300m...\n",
      "‚úÖ Local embedding model loaded successfully.\n",
      "üì• Loading local LLM: google/gemma-3-1b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local LLM loaded successfully.\n",
      "‚úÖ AI clients initialized.\n",
      "\n",
      "--- 2. Generating embeddings for all documents ---\n",
      "‚úÖ Generated 25 embeddings. Vector dimension: 768\n",
      "\n",
      "--- 3. Connecting to Weaviate ---\n",
      "‚úÖ Successfully connected to Weaviate.\n",
      "\n",
      "--- 4. Creating Weaviate collection: 'SimpleRAG' ---\n",
      "‚úÖ Collection 'SimpleRAG' created successfully.\n",
      "\n",
      "--- 5. Ingesting 25 documents into Weaviate ---\n",
      "‚úÖ Data ingestion complete. Total objects in collection: 25\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.util import generate_uuid5\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Wrapper class for the local Embeddings model ---\n",
    "class LocalHuggingFaceEmbeddings:\n",
    "    \"\"\"\n",
    "    This class adapts a local SentenceTransformer model\n",
    "    to the LangChain interface, which expects the methods embed_documents and embed_query.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        print(f\"üì• Loading local embedding model: {model_name}...\")\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            print(\"‚úÖ Local embedding model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {model_name}. Falling back to 'all-MiniLM-L6-v2'.\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        # Returns a list of lists\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        # Returns a single list\n",
    "        embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "        return embedding.tolist()\n",
    "\n",
    "\n",
    "# --- Wrapper class for the local LLM ---\n",
    "class LocalHuggingFaceChatModel(Runnable):\n",
    "    \"\"\"\n",
    "    A simple wrapper around the Transformers Pipeline to make it compatible\n",
    "    with LangChain's 'invoke' method and the pipe '|' operator.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        print(f\"üì• Loading local LLM: {model_name}...\")\n",
    "        # This is the 'Automatic Transmission' setup we discussed:\n",
    "        # 1. device=-1 forces CPU usage.\n",
    "        # 2. torch_dtype=torch.float32 is the fastest format for CPU.\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            device=-1,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        print(\"‚úÖ Local LLM loaded successfully.\")\n",
    "\n",
    "    def invoke(self, input_data, config: RunnableConfig = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Adapts LangChain inputs (PromptValue or Messages) to the pipeline format.\n",
    "        \"\"\"\n",
    "        # 1. Convert LangChain input to the list-of-dicts format expected by the pipeline\n",
    "        messages = []\n",
    "\n",
    "        # Handle LangChain PromptValue (which has .to_messages())\n",
    "        if hasattr(input_data, 'to_messages'):\n",
    "            lc_messages = input_data.to_messages()\n",
    "            for msg in lc_messages:\n",
    "                # Map LangChain message types to role strings\n",
    "                role = \"user\"\n",
    "                if msg.type == \"system\": role = \"system\"\n",
    "                elif msg.type == \"ai\": role = \"assistant\"\n",
    "\n",
    "                # Gemma pipeline expects content as a list of dicts or string.\n",
    "                messages.append({\"role\": role, \"content\": [{\"type\": \"text\", \"text\": msg.content}]})\n",
    "\n",
    "        # Handle raw string input (fallback)\n",
    "        elif isinstance(input_data, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": input_data}]}]\n",
    "\n",
    "        # 2. Run the pipeline (\"Automatic Transmission\")\n",
    "        # We set max_new_tokens to limit the answer length\n",
    "        outputs = self.pipe(messages, max_new_tokens=512)\n",
    "\n",
    "        # 3. Extract the generated text\n",
    "        # The pipeline returns a list of dicts. The last message is the assistant's reply.\n",
    "        generated_text = outputs[0]['generated_text'][-1]['content']\n",
    "\n",
    "        # 4. Return as an AIMessage to satisfy LangChain's StrOutputParser\n",
    "        return AIMessage(content=generated_text)\n",
    "\n",
    "\n",
    "# --- 1. Setup LangChain Clients ---\n",
    "print(\"--- 1. Setting up AI clients ---\")\n",
    "try:\n",
    "    # Embedding Model Setup\n",
    "    if USE_LOCAL_EMBEDDINGS:\n",
    "        embeddings_model = LocalHuggingFaceEmbeddings(LOCAL_EMBEDDING_MODEL_NAME)\n",
    "    else:\n",
    "        embeddings_model = AzureOpenAIEmbeddings(\n",
    "            azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"],\n",
    "            openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            dimensions=256  # size of embedding vectors, default is 1536\n",
    "        )\n",
    "\n",
    "    # Chat Model Setup\n",
    "    if USE_LOCAL_LLM:\n",
    "        chat_model = LocalHuggingFaceChatModel(LOCAL_LLM_MODEL_NAME)\n",
    "    else:\n",
    "        chat_model = AzureChatOpenAI(\n",
    "            azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n",
    "            openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            temperature=0\n",
    "        )\n",
    "    print(\"‚úÖ AI clients initialized.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize AI clients. Please check your .env file or model names. Error: {e}\")\n",
    "    # Stop execution if clients fail to initialize\n",
    "    raise\n",
    "\n",
    "# --- 2. Generate Embeddings ---\n",
    "print(\"\\n--- 2. Generating embeddings for all documents ---\")\n",
    "contents_to_embed = [doc['content'] for doc in documents_data]\n",
    "vector_embeddings = embeddings_model.embed_documents(contents_to_embed)\n",
    "print(f\"‚úÖ Generated {len(vector_embeddings)} embeddings. Vector dimension: {len(vector_embeddings[0])}\")\n",
    "\n",
    "# Add embeddings to our data\n",
    "for i, doc in enumerate(documents_data):\n",
    "    doc['content_vector'] = vector_embeddings[i]\n",
    "\n",
    "# --- 3. Connect to Weaviate ---\n",
    "print(\"\\n--- 3. Connecting to Weaviate ---\")\n",
    "weaviate_client = weaviate.connect_to_local(\n",
    "    host=\"localhost\",\n",
    "    port=WEAVIATE_HTTP_PORT,\n",
    "    grpc_port=WEAVIATE_GRPC_PORT\n",
    ")\n",
    "if weaviate_client.is_ready():\n",
    "    print(\"‚úÖ Successfully connected to Weaviate.\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to Weaviate.\")\n",
    "    weaviate_client.close()\n",
    "    raise ConnectionError(\"Could not connect to Weaviate instance.\")\n",
    "\n",
    "# --- 4. Define and Create Weaviate Collection ---\n",
    "COLLECTION_NAME = \"SimpleRAG\"\n",
    "print(f\"\\n--- 4. Creating Weaviate collection: '{COLLECTION_NAME}' ---\")\n",
    "\n",
    "# Delete collection if it already exists for a clean run\n",
    "if weaviate_client.collections.exists(COLLECTION_NAME):\n",
    "    weaviate_client.collections.delete(COLLECTION_NAME)\n",
    "    print(f\"Deleted existing collection '{COLLECTION_NAME}'.\")\n",
    "\n",
    "# Create new DB schema for our documents\n",
    "rag_collection = weaviate_client.collections.create(\n",
    "    name=COLLECTION_NAME,\n",
    "    properties=[\n",
    "        wvc.config.Property(name=\"title\", data_type=wvc.config.DataType.TEXT),\n",
    "        wvc.config.Property(name=\"content\", data_type=wvc.config.DataType.TEXT),\n",
    "    ],\n",
    "    # Deprecated config:\n",
    "    # vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    "    # vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "    #     distance_metric=wvc.config.VectorDistances.COSINE\n",
    "    # )\n",
    "    # Changes:\n",
    "    # - Renamed: vectorizer_config -> vector_config\n",
    "    # - Replaced: Configure.Vectorizer.none() -> Configure.Vectors.self_provided()\n",
    "    # - vector_index_config -> subargument of vector_config\n",
    "    # New config for client v4.18.1:\n",
    "    vector_config=wvc.config.Configure.Vectors.self_provided(\n",
    "        vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "            distance_metric=wvc.config.VectorDistances.COSINE\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(f\"‚úÖ Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "\n",
    "# --- 5. Batch-Insert Data ---\n",
    "print(f\"\\n--- 5. Ingesting {len(documents_data)} documents into Weaviate ---\")\n",
    "\n",
    "# Use a context manager to automatically handle batching\n",
    "with rag_collection.batch.dynamic() as batch:\n",
    "    for doc in documents_data:\n",
    "        properties = {\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": doc[\"content\"]\n",
    "        }\n",
    "        batch.add_object(\n",
    "            properties=properties,\n",
    "            vector=doc[\"content_vector\"],  # Use default vector\n",
    "            uuid=generate_uuid5(doc[\"title\"])  # Generate a consistent UUID based on the title\n",
    "        )\n",
    "\n",
    "print(f\"‚úÖ Data ingestion complete. Total objects in collection: {len(rag_collection)}\")\n",
    "\n",
    "# Close the client connection\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-a9b0-4c1d-2e3f-4a5b6c7d8e9f",
   "metadata": {},
   "source": [
    "## 7. RAG Experiment\n",
    "\n",
    "Now we perform the core RAG experiment. For each of our five topics, we will ask a question and follow the RAG pipeline to generate an answer.\n",
    "\n",
    "**The Pipeline:**\n",
    "1.  **Expand Query**: Use an LLM to rephrase the user's simple question into a richer, more descriptive query.\n",
    "2.  **Embed Query**: Generate a vector embedding for the expanded query.\n",
    "3.  **Retrieve Documents**: Search Weaviate for the top 5 documents most similar to the query vector.\n",
    "4.  **Generate Answer**: Pass the retrieved documents as context to another LLM call and ask it to synthesize a final, bulleted answer based *only* on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9-b0c1-4d2e-3f4a-5b6c7d8e9f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing query for topic: Atacama Desert\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Original Question: What do you know about the ALMA observatory?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rephrased Query for Search:** What are the key features, operational details, recent research findings, and publicly available data pertaining to the ALMA (Atacama Large Millimeter/submillimeter Array) observatory, including its location, instrumentation, data processing techniques, and scientific goals?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Top 5 Retrieved Documents:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retrieved Title</th>\n",
       "      <th>Cosine Distance</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stargazing Paradise</td>\n",
       "      <td>0.3978</td>\n",
       "      <td>0</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mars-like Soil</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rain in the Driest Place</td>\n",
       "      <td>0.7114</td>\n",
       "      <td>0</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nitrate Mining History</td>\n",
       "      <td>0.7676</td>\n",
       "      <td>0</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ancient Mummies</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>0</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Retrieved Title  Cosine Distance  Topic Correct\n",
       "0       Stargazing Paradise           0.3978      0       ‚úÖ\n",
       "1            Mars-like Soil           0.6486      0       ‚úÖ\n",
       "2  Rain in the Driest Place           0.7114      0       ‚úÖ\n",
       "3    Nitrate Mining History           0.7676      0       ‚úÖ\n",
       "4           Ancient Mummies           0.7745      0       ‚úÖ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, no RAG:**\n",
       "Here's a summary of the ALMA observatory based solely on the provided context:\n",
       "\n",
       "*   ALMA stands for Atacama Large Millimeter/submillimeter Array.\n",
       "*   It‚Äôs a collaborative project involving several European and Chilean institutions.\n",
       "*   It‚Äôs located in the Atacama Desert, Chile.\n",
       "*   ALMA is designed to observe the universe at millimeter and submillimeter wavelengths.\n",
       "*   It‚Äôs known for its high-resolution imaging capabilities.\n",
       "*   It‚Äôs used to study star formation, molecular clouds, and the early universe."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, with RAG:**\n",
       "Here‚Äôs a summary of the provided context regarding the ALMA observatory:\n",
       "\n",
       "*   The Atacama Large Millimeter Array (ALMA) is a major observatory located in the Atacama Desert.\n",
       "*   It consists of 66 high-precision antennas.\n",
       "*   NASA used the Atacama Desert as a testing ground for instruments intended for Mars missions, including the Sample Analysis at Mars (SAM) instrument suite.\n",
       "*   The Atacama Desert is the driest nonpolar desert in the world.\n",
       "*   In 2015, a rare weather event caused the desert to bloom with flowers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing query for topic: Tardigrades\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Original Question: Tell me about the Dsup protein in water bears."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rephrased Query for Search:** ‚ÄúDescribe the known function, expression patterns, and recent research related to the Dsup protein, specifically focusing on its role in water bear (specifically, *Phasmatodea*) specimens, including any observed effects on water bear behavior or physiology.‚Äù\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Top 5 Retrieved Documents:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retrieved Title</th>\n",
       "      <th>Cosine Distance</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radiation Resistance</td>\n",
       "      <td>0.5935</td>\n",
       "      <td>1</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremophile Survivors</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>1</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anhydrobiosis</td>\n",
       "      <td>0.7534</td>\n",
       "      <td>1</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microscopic Size</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>1</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Space Travelers</td>\n",
       "      <td>0.8185</td>\n",
       "      <td>1</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Retrieved Title  Cosine Distance  Topic Correct\n",
       "0    Radiation Resistance           0.5935      1       ‚úÖ\n",
       "1  Extremophile Survivors           0.6505      1       ‚úÖ\n",
       "2           Anhydrobiosis           0.7534      1       ‚úÖ\n",
       "3        Microscopic Size           0.7555      1       ‚úÖ\n",
       "4         Space Travelers           0.8185      1       ‚úÖ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, no RAG:**\n",
       "I‚Äôm sorry, but the provided context does not contain the answer to this question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, with RAG:**\n",
       "Here‚Äôs a bullet-point summary of the provided context regarding the Dsup protein:\n",
       "\n",
       "*   Dsup is a unique protein found in tardigrades.\n",
       "*   It protects tardigrade DNA from radiation damage.\n",
       "*   The protein is known as ‚ÄúDamage suppressor.‚Äù\n",
       "*   Tardigrades can withstand radiation doses hundreds of times higher than lethal to humans."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing query for topic: Magnus Effect\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Original Question: What was the Buckau ship and its 1926 journey?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rephrased Query for Search:** ‚ÄúIdentify all records related to the Buckau ship, specifically focusing on its voyage in 1926. Include details about the ship‚Äôs name, route, destination, and any associated events or notable circumstances during that journey.‚Äù\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Top 5 Retrieved Documents:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retrieved Title</th>\n",
       "      <th>Cosine Distance</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flettner Rotors</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>2</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia's Unusual Conflict</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>3</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Second Attempt</td>\n",
       "      <td>0.8469</td>\n",
       "      <td>3</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Origin of the Name</td>\n",
       "      <td>0.8719</td>\n",
       "      <td>4</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Examples in Daily Life</td>\n",
       "      <td>0.8773</td>\n",
       "      <td>4</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Retrieved Title  Cosine Distance  Topic Correct\n",
       "0               Flettner Rotors           0.6239      2       ‚úÖ\n",
       "1  Australia's Unusual Conflict           0.8382      3       ‚ùå\n",
       "2              A Second Attempt           0.8469      3       ‚ùå\n",
       "3            Origin of the Name           0.8719      4       ‚ùå\n",
       "4        Examples in Daily Life           0.8773      4       ‚ùå"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, no RAG:**\n",
       "The provided context does not contain the answer to this question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, with RAG:**\n",
       "Here‚Äôs a bullet-point summary of the provided context regarding the Buckau ship:\n",
       "\n",
       "*   The Buckau was a ship that successfully crossed the Atlantic in 1926.\n",
       "*   It utilized two large rotating cylinders powered by the Magnus effect for propulsion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing query for topic: Great Emu War\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Original Question: What kind of weapons did Major Meredith's forces use?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rephrased Query for Search:** What types of firearms, explosives, and other weaponry were employed by Major Meredith‚Äôs military forces during the Battle of Blackwood Creek, specifically focusing on the tactical deployment and characteristics of their arsenal?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Top 5 Retrieved Documents:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retrieved Title</th>\n",
       "      <th>Cosine Distance</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Military Hardware</td>\n",
       "      <td>0.4927</td>\n",
       "      <td>3</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia's Unusual Conflict</td>\n",
       "      <td>0.6518</td>\n",
       "      <td>3</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Elusive Emus</td>\n",
       "      <td>0.6932</td>\n",
       "      <td>3</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operation Outcome</td>\n",
       "      <td>0.7932</td>\n",
       "      <td>3</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Microscopic Size</td>\n",
       "      <td>0.8278</td>\n",
       "      <td>1</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Retrieved Title  Cosine Distance  Topic Correct\n",
       "0             Military Hardware           0.4927      3       ‚úÖ\n",
       "1  Australia's Unusual Conflict           0.6518      3       ‚úÖ\n",
       "2              The Elusive Emus           0.6932      3       ‚úÖ\n",
       "3             Operation Outcome           0.7932      3       ‚úÖ\n",
       "4              Microscopic Size           0.8278      1       ‚ùå"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, no RAG:**\n",
       "The provided context does not contain the answer to this question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, with RAG:**\n",
       "Here‚Äôs a bullet-point summary of the information provided:\n",
       "\n",
       "*   Major G.P.W. Meredith‚Äôs military force used two Lewis automatic machine guns.\n",
       "*   The force was equipped with 10,000 rounds of ammunition."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing query for topic: Baader-Meinhof Phenomenon\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Original Question: What is the origin of the name for the frequency illusion?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rephrased Query for Search:** ‚ÄúExplore the historical and psychological origins of the ‚Äúfrequency illusion‚Äù name, tracing its development from initial observation to its current usage in research and popular discussion. Focus specifically on the debates surrounding the name‚Äôs connection to Benjamin Lee Wesp, the pioneering researcher who coined the term, and the evolution of the concept‚Äôs meaning.‚Äù\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Top 5 Retrieved Documents:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retrieved Title</th>\n",
       "      <th>Cosine Distance</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frequency Illusion</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>4</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not a Real Increase</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>4</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two Cognitive Processes</td>\n",
       "      <td>0.6275</td>\n",
       "      <td>4</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Origin of the Name</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>4</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Examples in Daily Life</td>\n",
       "      <td>0.7405</td>\n",
       "      <td>4</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Retrieved Title  Cosine Distance  Topic Correct\n",
       "0       Frequency Illusion           0.4924      4       ‚úÖ\n",
       "1      Not a Real Increase           0.5950      4       ‚úÖ\n",
       "2  Two Cognitive Processes           0.6275      4       ‚úÖ\n",
       "3       Origin of the Name           0.6711      4       ‚úÖ\n",
       "4   Examples in Daily Life           0.7405      4       ‚úÖ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, no RAG:**\n",
       "The provided context does not contain the answer to this question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the original query, with RAG:**\n",
       "*   The name originated in 1994 when a commenter on the St. Paul Pioneer Press online board mentioned hearing about the German Baader-Meinhof Gang twice in 24 hours.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Re-connect to Weaviate for the experiment\n",
    "weaviate_client = weaviate.connect_to_local(\n",
    "    host=\"localhost\",\n",
    "    port=WEAVIATE_HTTP_PORT,\n",
    "    grpc_port=WEAVIATE_GRPC_PORT\n",
    ")\n",
    "rag_collection = weaviate_client.collections.get(COLLECTION_NAME)\n",
    "\n",
    "# --- Define Test Questions ---\n",
    "test_questions = [\n",
    "    {\"topic\": \"Atacama Desert\", \"question\": \"What do you know about the ALMA observatory?\"},\n",
    "    {\"topic\": \"Tardigrades\", \"question\": \"Tell me about the Dsup protein in water bears.\"},\n",
    "    {\"topic\": \"Magnus Effect\", \"question\": \"What was the Buckau ship and its 1926 journey?\"},\n",
    "    {\"topic\": \"Great Emu War\", \"question\": \"What kind of weapons did Major Meredith's forces use?\"},\n",
    "    {\"topic\": \"Baader-Meinhof Phenomenon\", \"question\": \"What is the origin of the name for the frequency illusion?\"}\n",
    "]\n",
    "\n",
    "# --- Create Mappings for Topic Validation ---\n",
    "# This mapping links each document title to a topic index (0-4)\n",
    "# It assumes documents_data is defined in a previous cell and grouped by 5.\n",
    "title_to_topic_index = {doc['title']: i // 5 for i, doc in enumerate(documents_data)}\n",
    "# This mapping links the topic name from our test questions to the same index\n",
    "topic_name_to_index = {q['topic']: i for i, q in enumerate(test_questions)}\n",
    "\n",
    "\n",
    "# --- Define LangChain Chains ---\n",
    "\n",
    "# 1. Chain for Query Expansion\n",
    "expansion_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert in information retrieval. \"\n",
    "    \"Please rephrase the following user query to be more descriptive and detailed, \"\n",
    "    \"making it suitable for a vector database search. \"\n",
    "    \"Return only the rephrased query, without any additional text, headers, or explanations. \"\n",
    "    \"\\n\\nOriginal Query: '{query}'\\n\\nRephrased Query:\"\n",
    ")\n",
    "query_expansion_chain = expansion_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# 2. Chain for Final Answer Generation (with RAG context)\n",
    "generation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a factual assistant. \"\n",
    "    \"Your task is to answer the user's question based only on the provided context, \"\n",
    "    \"do not use common knowledge, do not correct mistakes in provided context. \"\n",
    "    \"Synthesize the information from the context into a concise, bullet-point summary. \"\n",
    "    \"Focus on specific details like names, numbers, and technical terms mentioned in the context. \"\n",
    "    \"If the context does not contain the information needed to answer the question, \"\n",
    "    \"you must state: 'The provided context does not contain the answer to this question.' \"\n",
    "    \"\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "answer_generation_chain = generation_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "\n",
    "# --- Run the Experiment ---\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for item in test_questions:\n",
    "    user_query = item[\"question\"]\n",
    "    current_topic_name = item[\"topic\"]\n",
    "    current_topic_index = topic_name_to_index[current_topic_name]\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\\nProcessing query for topic: {current_topic_name}\\n{'=' * 80}\")\n",
    "    display(Markdown(f\"### Original Question: {user_query}\"))\n",
    "\n",
    "    # 1. Expand the query\n",
    "    expanded_query = query_expansion_chain.invoke({\"query\": user_query})\n",
    "    display(Markdown(f\"**Rephrased Query for Search:** {expanded_query}\"))\n",
    "\n",
    "    # --- RAG Pipeline ---\n",
    "\n",
    "    # Embed Expanded Query\n",
    "    query_embedding = embeddings_model.embed_query(expanded_query)\n",
    "\n",
    "    # Retrieve Documents from Weaviate\n",
    "    retrieved_objects = rag_collection.query.near_vector(\n",
    "        near_vector=query_embedding,\n",
    "        limit=5,\n",
    "        return_metadata=wvc.query.MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    retrieved_docs_content = [obj.properties['content'] for obj in retrieved_objects.objects]\n",
    "    context_for_llm = \"\\n\\n---\\n\\n\".join(retrieved_docs_content)\n",
    "\n",
    "    # Display retrieved documents with Topic and Correctness Check\n",
    "    retrieved_titles = [obj.properties['title'] for obj in retrieved_objects.objects]\n",
    "    retrieved_distances = [round(obj.metadata.distance, 4) for obj in retrieved_objects.objects]\n",
    "    retrieved_topics = [title_to_topic_index.get(title, -1) for title in retrieved_titles]\n",
    "    retrieved_checks = ['‚úÖ' if topic_idx == current_topic_index else '‚ùå' for topic_idx in retrieved_topics]\n",
    "\n",
    "    df_retrieved = pd.DataFrame({\n",
    "        'Retrieved Title': retrieved_titles,\n",
    "        'Cosine Distance': retrieved_distances,\n",
    "        'Topic': retrieved_topics,\n",
    "        'Correct': retrieved_checks\n",
    "    })\n",
    "    display(Markdown(\"**Top 5 Retrieved Documents:**\"))\n",
    "    display(df_retrieved)\n",
    "\n",
    "    # 2. Get answer from the model on the ORIGINAL question (without RAG) just for comparizon\n",
    "    answer_no_rag_original = answer_generation_chain.invoke({\n",
    "        \"context\": \"No context found.\",\n",
    "        \"question\": user_query\n",
    "    })\n",
    "    display(Markdown(f\"**Answer to the original query, no RAG:**\\n{answer_no_rag_original}\"))\n",
    "    \n",
    "    # 3. Generate Final Answer using RAG\n",
    "    final_answer = answer_generation_chain.invoke({\n",
    "        \"context\": context_for_llm,\n",
    "        \"question\": user_query\n",
    "    })\n",
    "    display(Markdown(f\"**Answer to the original query, with RAG:**\\n{final_answer}\"))\n",
    "\n",
    "    experiment_results.append({\n",
    "        \"topic\": item[\"topic\"],\n",
    "        \"original_query\": user_query,\n",
    "        \"expanded_query\": expanded_query,\n",
    "        \"retrieved_docs\": retrieved_titles,\n",
    "        \"final_answer\": final_answer\n",
    "    })\n",
    "\n",
    "# Close the client connection\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0-c1d2-4e3f-4a5b-6c7d8e9f0a1b",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "The experiment is complete. The final step is to stop and remove the Weaviate Docker container to free up system resources. The Docker image will be kept for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1-d2e3-4f4a-5b6c-7d8e9f0a1b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stopping and removing container 'simple-rag-weaviate' ---\n",
      "‚ö†Ô∏è Container may have already been stopped or removed.\n",
      "Details: <3>WSL (555 - Relay) ERROR: CreateProcessCommon:800: execvpe(bash) failed: No such file or directory\n",
      "\n",
      "--- Docker container status after cleanup ---\n",
      "Container status:\n",
      "\n",
      "Container status:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Stopping and removing container '{WEAVIATE_CONTAINER_NAME}' ---\")\n",
    "\n",
    "# Windows PowerShell uses semicolon instead of &&\n",
    "if platform.system() == \"Windows\":\n",
    "    cleanup_command = f\"docker stop {WEAVIATE_CONTAINER_NAME}; docker rm {WEAVIATE_CONTAINER_NAME}\"\n",
    "else:\n",
    "    cleanup_command = f\"docker stop {WEAVIATE_CONTAINER_NAME} && docker rm {WEAVIATE_CONTAINER_NAME}\"\n",
    "\n",
    "result = run_shell_command(cleanup_command)\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(f\"‚úÖ Container stopped and removed successfully.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Container may have already been stopped or removed.\")\n",
    "    print(f\"Details: {result['stderr']}\")\n",
    "\n",
    "print(\"\\n--- Docker container status after cleanup ---\")\n",
    "ps_result = run_shell_command(f\"docker ps -a --filter name={WEAVIATE_CONTAINER_NAME}\")\n",
    "if ps_result[\"success\"] and WEAVIATE_CONTAINER_NAME not in ps_result[\"stdout\"]:\n",
    "    print(\"‚úÖ No container found. Cleanup confirmed.\")\n",
    "else:\n",
    "    print(\"Container status:\")\n",
    "    print(ps_result[\"stdout\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2732e5-c2da-41a5-b3ce-3041cf155b1a",
   "metadata": {},
   "source": [
    "# Additional: running models locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71b6ed-e5ea-42aa-8113-a87c9571308d",
   "metadata": {},
   "source": [
    "## 9. Try local run for embeddings model\n",
    "\n",
    "**NOTE**: you had to register at hugging face and accept embeddiggemma 3 license on site to use this approach.\n",
    "\n",
    "References:\n",
    "- https://developers.googleblog.com/en/introducing-embeddinggemma/\n",
    "- https://huggingface.co/google/embeddinggemma-300m"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46911719-948b-4aaa-98d0-877316af7794",
   "metadata": {},
   "source": [
    "!\"{sys.executable}\" -m pip install -U -q sentence-transformers\n",
    "\n",
    "print(\"‚úÖ Extra libraries have been installed.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cf61313-89df-46c1-8cc4-d63622dec3d8",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download from the ü§ó Hub\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# Run inference with queries and documents\n",
    "query = \"Which planet is known as the Red Planet?\"\n",
    "documents = [\n",
    "    \"Venus is often called Earth's twin because of its similar size and proximity.\",\n",
    "    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n",
    "    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n",
    "    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n",
    "]\n",
    "query_embeddings = model.encode_query(query)\n",
    "document_embeddings = model.encode_document(documents)\n",
    "print(query_embeddings.shape, document_embeddings.shape)\n",
    "# (768,) (4, 768)\n",
    "\n",
    "# Compute similarities to determine a ranking\n",
    "similarities = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarities)\n",
    "# tensor([[0.3011, 0.6359, 0.4930, 0.4889]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2a18d-ffcd-464b-9c08-4d4155246b5f",
   "metadata": {},
   "source": [
    "## 10. Try local run for text generation model\n",
    "\n",
    "**NOTE**: you had to register at hugging face site to use this approach.\n",
    "\n",
    "**CODE**: below there are two examples of the same. The first is like a car with \"automatic transmission\", and the second is \"manual transmission\", that may be more suitable for study.\n",
    "\n",
    "References:\n",
    "- https://huggingface.co/google/gemma-3-1b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fee446-bf55-4b6c-a5ea-ce4b80273bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Write a poem on Hugging Face, the company'}]}, {'role': 'assistant', 'content': 'Okay, here‚Äôs a poem about Hugging Face, aiming to capture its essence and feel:\\n\\n**The Neural Forge**\\n\\nWithin the cloud, a vibrant hue,\\nHugging Face, a digital view.\\nA community, a steady'}]}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-it\", # Using a 1B or 2B parameter model is optimal for a laptop. It consumes less RAM (approx 2-4GB) and generates text reasonably fast on a CPU.\n",
    "    device=-1,                    # Set device=\"cuda\" to -1 to tell Transformers to use the CPU           \n",
    "    torch_dtype=torch.float32     # Float32 is the native and fastest format for CPUs, bfloat16 work slower or unsupported.\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1cfb9-a496-4238-b42d-ec595a59572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company\n",
      "model\n",
      "Okay, here‚Äôs a poem about Hugging Face, aiming to capture its essence and feel:\n",
      "\n",
      "**The Neural Bloom**\n",
      "\n",
      "In a world of code, a digital space,\n",
      "Where models grow, with elegant grace,\n",
      "Lies Hugging Face, a vibrant hue,\n",
      "A community built, both old\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# REMOVED: BitsAndBytesConfig\n",
    "# Quantization (load_in_8bit) relies on CUDA (GPU) and does not work on CPU.\n",
    "# Since the 1B model is small, we can load it normally into RAM without quantization.\n",
    "\n",
    "# Load the model explicitly for CPU usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,    # Use float32 for best CPU performance and compatibility\n",
    "    device_map=\"cpu\"              # Explicitly load model to CPU\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Note: apply_chat_template expects a single list of messages for a single prompt, \n",
    "# or a list of lists for batch processing. Your structure implies a batch of 1.\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cpu\") # Move inputs to CPU. \n",
    "# REMOVED: .to(torch.bfloat16). Inputs for the model (input_ids) are integers, \n",
    "# and attention_mask is usually handled automatically. Converting input_ids to float causes errors.\n",
    "\n",
    "# Generate output\n",
    "with torch.inference_mode(): # inference_mode is slightly faster than no_grad\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "# Decode and print\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded_output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
